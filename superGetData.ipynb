{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "superGetData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azarenkovgd/consents-recognition/blob/master/superGetData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpj0jZYLlqKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa09af5b-9c60-4745-e631-534490ab0097"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import os\r\n",
        "os.chdir('/content/drive/My Drive/iinti')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZU9o9QmHvr"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY1f5QvKmD19"
      },
      "source": [
        "!pip install pymorphy2\r\n",
        "!pip install pymystem3\r\n",
        "!pip install tensorflow-gpu==1.15.2\r\n",
        "!pip install pandas==1.0.0\r\n",
        "!pip install jsonlines\r\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY_Hjil1ME5U",
        "outputId": "a0f73028-054b-4114-ec4f-730c1eb43ecd"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgn8bUH3rH52"
      },
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import gc\r\n",
        "import json\r\n",
        "import nltk\r\n",
        "import pymorphy2\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "\r\n",
        "from pymystem3 import Mystem\r\n",
        "from nltk.corpus import words\r\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIqF1UaGKx9"
      },
      "source": [
        "import json\r\n",
        "import jsonlines\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quNK2rVlF8Mt"
      },
      "source": [
        "#GetData \r\n",
        "\r\n",
        "*   (Просто решил сделать в одном файле для удобства)\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWuDgEWPF6d1"
      },
      "source": [
        "!pip install jsonlines\r\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA80jS5vGJMu"
      },
      "source": [
        "def load_json(path: str):\r\n",
        "    with open(path, 'r', encoding='utf-8') as read_file:\r\n",
        "        data = json.load(read_file)\r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMbyMEDtGNRf"
      },
      "source": [
        "with open('data/test.jsonl', 'r') as json_file:\r\n",
        "        json_list = list(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9BBPz88GYEa"
      },
      "source": [
        "def get_X_y(data_json_file):\r\n",
        "    X, y = [], []\r\n",
        "    with open(data_json_file, 'r') as json_file:\r\n",
        "        json_list = list(json_file)\r\n",
        "        \r\n",
        "        for json_str in json_list:\r\n",
        "            item = json.loads(json_str)\r\n",
        "            text = item['passage']['text']\r\n",
        "            \r\n",
        "            questions = item['passage']['questions']\r\n",
        "            for q in questions:\r\n",
        "                query = q['question']\r\n",
        "                ans = q['answers']\r\n",
        "                for a in ans:\r\n",
        "                    X.append(text + query + a['text'])\r\n",
        "                    y.append(a['label'])\r\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vv7_JujG6UJ"
      },
      "source": [
        "X_train, y_train = get_X_y('data/train.jsonl')\r\n",
        "X_test, y_test = get_X_y('data/val.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-ASpMMG9Rp"
      },
      "source": [
        "train_df = pd.DataFrame({\r\n",
        "    'text': X_train,\r\n",
        "    'labels':y_train\r\n",
        "})\r\n",
        "\r\n",
        "eval_df = pd.DataFrame({\r\n",
        "    'text': X_test,\r\n",
        "    'labels': y_test\r\n",
        "})\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpp4HudwpC5o"
      },
      "source": [
        "# Функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njjC6vU0mbQJ"
      },
      "source": [
        "def save_csv(df,path_out, sep=',', encoding='utf-8', index=False, header=True):\r\n",
        "    #Сохраняем CSV-файл\r\n",
        "    path_out = path_out.replace('\\\\','/')\r\n",
        "\r\n",
        "    if not os.path.exists('/'.join(path_out.split('/')[0:-1])):\r\n",
        "        os.makedirs('/'.join(path_out.split('/')[0:-1]))\r\n",
        "        \r\n",
        "    df.to_csv(path_out, index=index, header=header, sep=sep, encoding=encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6aOXvSwmtUE"
      },
      "source": [
        "def fillna(df, column_str, replace_to=''):\r\n",
        "    #Заполнение пропусков\r\n",
        "\r\n",
        "    df[column_str] = df[column_str].fillna(replace_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi1TC4JOm3Mc"
      },
      "source": [
        "def clear_chars(text):\r\n",
        "    #Удаляем лишние символы, включая лишние пробелы и переносы строк\r\n",
        "\r\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я 0-9]+', ' ', text) \r\n",
        "    return ' '.join(text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeV4ZYNgm-PR"
      },
      "source": [
        "def do_lower(text):\r\n",
        "    #Приводим текст к нижнему регистру\r\n",
        "\r\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FM_oM1XnDjS"
      },
      "source": [
        "def creat_stop_words(df, text_str, label_str, stop_words_excep, stop_words_extra, min_labels_count=None):\r\n",
        "    #Формируем список стоп-слов (слова которые встречаются в заданном числе меток)\r\n",
        "    # min_labels_count - минимальное кол-во категорий в которых должно содержаться стоп-слово\r\n",
        "\r\n",
        "    # Список меток\r\n",
        "    labels = list(set(df[label_str].to_list()))\r\n",
        "    \r\n",
        "    # Кол-во категорий в которых должно содержаться стоп-слово\r\n",
        "    if min_labels_count is None:\r\n",
        "        min_labels_count = len(labels)\r\n",
        "\r\n",
        "    #Список слов для каждой метки\r\n",
        "    labels_word = {}\r\n",
        "    for label in labels:\r\n",
        "        labels_word[label]= get_dict(df[df[label_str] == label],text_str)\r\n",
        "        \r\n",
        "    # Формируем список стоп-слов  \r\n",
        "    stop_words = []\r\n",
        "    uniq_words = get_dict(df,text_str)\r\n",
        "        \r\n",
        "    for word in uniq_words:\r\n",
        "        count = 0\r\n",
        "        for label in labels:\r\n",
        "            if word in labels_word[label]:\r\n",
        "                count+=1\r\n",
        "        if count >= min_labels_count and word not in stop_words_excep:\r\n",
        "            stop_words.append(word)\r\n",
        "    \r\n",
        "    stop_words.extend(stop_words_extra)\r\n",
        "    return stop_words "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ik1cjlnM7F"
      },
      "source": [
        "def delete_stopwords(text):\r\n",
        "    # Очистка от стоп-слов\r\n",
        "    # stops - список стоп слов. Если пустой то используем стандартные\r\n",
        "\r\n",
        "    global stop_words\r\n",
        "    if stop_words is None:\r\n",
        "        stop_words = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\r\n",
        "\r\n",
        "    new_sentence = [word for word in text.split() if not word.lower() in stop_words]\r\n",
        "    return ' '.join(new_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlvkSvM_nRQ8"
      },
      "source": [
        "def pymorphy_lemmatization_ru(text,fix_case=False,fix_aggressive=False):\r\n",
        "# Лемматизатор для русских текстов (pymorphy)\r\n",
        "\r\n",
        "        new_sentence = [morph_ru.parse(word)[0].normal_form for word in text.split()]\r\n",
        "        if fix_case:\r\n",
        "            fix_sentence = []\r\n",
        "            old_sentence = text.split()\r\n",
        "            for old,new in zip(old_sentence,new_sentence):\r\n",
        "              fix_sentence.append(fix_text_case(old,new,fix_aggressive))\r\n",
        "            return ' '.join(fix_sentence) \r\n",
        "        return ' '.join(new_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzxA2aNnnbLw"
      },
      "source": [
        "def pymystem_lemmatization_ru(text,fix_case=False,fix_aggressive=False):\r\n",
        "    # Лемматизатор для русских текстов (pymystem)\r\n",
        "\r\n",
        "        new_sentence = ''.join(lemmatizator.lemmatize(text)[:-1]).split()\r\n",
        "        if fix_case:\r\n",
        "            fix_sentence = []\r\n",
        "            old_sentence = text.split()\r\n",
        "            for old,new in zip(old_sentence,new_sentence):\r\n",
        "              fix_sentence.append(fix_text_case(old,new,fix_aggressive))\r\n",
        "            return ' '.join(fix_sentence) \r\n",
        "        return ' '.join(new_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlAVQimMnqpz"
      },
      "source": [
        "def do_lemmatization(text, fix_case=True, fix_aggressive=False):\r\n",
        "    # Лемматизация текста\r\n",
        "\r\n",
        "    if lemm_by == 'pymorphy':\r\n",
        "        text = pymorphy_lemmatization_ru(text,fix_case=fix_case,fix_aggressive=False)\r\n",
        "    else:\r\n",
        "        text = pymystem_lemmatization_ru(text,fix_case=fix_case,fix_aggressive=False)\r\n",
        "    return text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylI967wYnwZe"
      },
      "source": [
        "def load_spellFix_model(vocab_path=None,lang_model_path=None,download=False):\r\n",
        "    # Загружаем предобученную модель исправления опечаток\r\n",
        "\r\n",
        "    spell_model_config = json.load(open(configs.spelling_correction.levenshtein_corrector_ru))\r\n",
        "    \r\n",
        "    if vocab_path:\r\n",
        "        spell_model_config['chainer']['pipe'][2]['load_path'] = vocab_path\r\n",
        "        spell_model_config['chainer']['pipe'][2]['save_path'] = vocab_path\r\n",
        "\r\n",
        "    if lang_model_path:\r\n",
        "        spell_model_config['chainer']['pipe'][4]['load_path'] = lang_model_path\r\n",
        "        \r\n",
        "    return build_model(spell_model_config, download=download)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtka3MNfn34-"
      },
      "source": [
        "def fixSpell_ru(text,fix_case=True,fix_aggressive=False):\r\n",
        "    # Исправление опечаток в русских текстах\r\n",
        "\r\n",
        "    new_sentence = model_spell([text])[0]    \r\n",
        "    if fix_case:\r\n",
        "            return fix_text_case(text,new_sentence,fix_aggressive)\r\n",
        "    return new_sentence    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2_t_EQ3n8jY"
      },
      "source": [
        "def fix_keyboard_layout(word):\r\n",
        "    # Проверяем слово на корректность раскладки клавиатуры при его наборе\r\n",
        "    \r\n",
        "    def check_layot(word):\r\n",
        "        ru = 0\r\n",
        "        en = 0\r\n",
        "        for char in word:\r\n",
        "            if char in alphabet_ru:\r\n",
        "                ru+=1\r\n",
        "            if char in alphabet_en:\r\n",
        "                en+=1\r\n",
        "        return 'ru' if ru >= en else 'en'\r\n",
        "    \r\n",
        "    def switch_layot(word,to_layot):\r\n",
        "        new_word = ''\r\n",
        "        for char in word:\r\n",
        "            was_find = False\r\n",
        "            for ru,en in zip(layout_ru,layout_en):\r\n",
        "                if to_layot == 'en':\r\n",
        "                    if char == ru:\r\n",
        "                        new_word+= en\r\n",
        "                        was_find = True\r\n",
        "                        break\r\n",
        "                else:\r\n",
        "                    if char == en:\r\n",
        "                        new_word+= ru\r\n",
        "                        was_find = True\r\n",
        "                        break \r\n",
        "            if not was_find:\r\n",
        "                new_word+= char\r\n",
        "        return new_word                       \r\n",
        "        \r\n",
        "        \r\n",
        "    current_layot = check_layot(word)\r\n",
        "    if current_layot == 'ru':\r\n",
        "        russian = word_is_ru(word)\r\n",
        "    else:\r\n",
        "        english = word_is_en(word)\r\n",
        "        \r\n",
        "    if current_layot == 'ru' and russian:  \r\n",
        "        return word\r\n",
        "    elif current_layot == 'ru' and not russian:\r\n",
        "        word_en = switch_layot(word,'en')\r\n",
        "        english = word_is_en(word_en)\r\n",
        "        return word_en if english else word\r\n",
        "    elif current_layot == 'en' and english:\r\n",
        "            return word\r\n",
        "    elif current_layot == 'en' and not english:\r\n",
        "        word_ru = switch_layot(word,'ru')\r\n",
        "        russian = word_is_ru(word_ru)\r\n",
        "        return word_ru if russian else word    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wKPk1ZmoIjR"
      },
      "source": [
        "def tokenize_by_rules(string):\r\n",
        "# Токенизируем строку по заданной группе(правилам разделения)\r\n",
        "\r\n",
        "        token = ''\r\n",
        "        tokens = []\r\n",
        "        category = None\r\n",
        "        categories = ['0123456789','абвгдеёжзийклмнопрстуфхцчшщъыьэюя','abcdefghijklmnopqrstuvwxyz']\r\n",
        "        for char in string:\r\n",
        "            if token:\r\n",
        "                if category and char.lower() in category:\r\n",
        "                     token += char\r\n",
        "                else:\r\n",
        "                     if not token == ' ':\r\n",
        "                         tokens.append(token)\r\n",
        "                     token = char\r\n",
        "                     category = None\r\n",
        "                     for cat in categories:\r\n",
        "                         if char.lower() in cat:\r\n",
        "                             category = cat\r\n",
        "                             break\r\n",
        "            else:\r\n",
        "                 category = None\r\n",
        "                 if not category:\r\n",
        "                     for cat in categories:\r\n",
        "                         if char.lower() in cat:\r\n",
        "                             category = cat\r\n",
        "                             break\r\n",
        "                 token += char\r\n",
        "        if token:\r\n",
        "             if not token == ' ':\r\n",
        "                 tokens.append(token)\r\n",
        "        return ' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul23E8YQoaBZ"
      },
      "source": [
        "def replace_empty_to_max_freq_label(df,text_str,label_str,empty_str=''):\r\n",
        "# Определяем наиболее частую метку для пропущенных данных\r\n",
        "# и присваиваем её для всех попусков.\r\n",
        "\r\n",
        "        df_empty = df[df[text_str] == empty_str]\r\n",
        "        df_empty_uniq = pd.value_counts(df_empty.values.ravel())[1:]\r\n",
        "        try:\r\n",
        "            max_freq_label = df_empty_uniq.index[0]\r\n",
        "\r\n",
        "            df_empty_indx = df_empty.index.to_list()\r\n",
        "            for empty_indx in df_empty_indx:\r\n",
        "                df.loc[empty_indx,label_str] = max_freq_label\r\n",
        "        except:\r\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1X3zQz5orHU"
      },
      "source": [
        "def word_is_en(word):\r\n",
        "        return word.lower() in vocab_en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THbMSV0rot1d"
      },
      "source": [
        "def word_is_ru(word):\r\n",
        "        return morph_ru.word_is_known(word.lower(), strict_ee=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YpsumBfozQd"
      },
      "source": [
        "def get_list_of_unknow_words(words_list):\r\n",
        "        \"\"\" Список неизвестных слов\r\n",
        "        \"\"\"\r\n",
        "        unknow = []\r\n",
        "        for word in words_list:\r\n",
        "            if not word_is_ru(word) and not word_is_en(word):\r\n",
        "                unknow.append(word)\r\n",
        "        return unknow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyvbTNEho1Xn"
      },
      "source": [
        "def get_dict(df,column_str='',uniq=True):\r\n",
        "        \"\"\" Список уникальных слов в конкретном столбце\r\n",
        "        \"\"\"\r\n",
        "        if not isinstance(df,list):\r\n",
        "            rows = df[column_str].to_list()\r\n",
        "        else:\r\n",
        "            rows = df\r\n",
        "        dictionary = []\r\n",
        "        for row in rows:\r\n",
        "            dictionary.extend(row.split())\r\n",
        "        return list(set(dictionary)) if uniq else dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHsmwNnw64wt"
      },
      "source": [
        "def save_Dataset(dataset,folder_path,sep=',',encoding='utf-8',index=False,header=True,postfix=''):\r\n",
        "        \"\"\" Сохраняем датасет\r\n",
        "        \"\"\"  \r\n",
        "        if 'train' in dataset:\r\n",
        "            save_csv(dataset['train'], os.path.join(folder_path, \"train\" + postfix + \".csv\"),\r\n",
        "                    sep=sep,encoding=encoding,index=index,header=header)\r\n",
        "        if 'test' in dataset:\r\n",
        "            save_csv(dataset['test'], os.path.join(folder_path, \"test\" + postfix + \".csv\"),\r\n",
        "                    sep=sep,encoding=encoding,index=index,header=header)\r\n",
        "        if 'valid' in dataset:\r\n",
        "            save_csv(dataset['valid'], os.path.join(folder_path, \"valid\" + postfix + \".csv\"),\r\n",
        "                    sep=sep,encoding=encoding,index=index,header=header)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WODo2fjMpLEv"
      },
      "source": [
        "#Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l6GIAd4PgvX"
      },
      "source": [
        "### Провести нормальную аналитику (задача)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxYXn9RgpOyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7d356f-1fdb-4bdd-b6a9-a414e4469274"
      },
      "source": [
        "stop_words = None\r\n",
        "stop_words_excep = ['интернет','оператор','тариф','интернета','работает','нет']\r\n",
        "stop_words_extra = ['добрый','доброго','добры','доброй','доброе','добрые',\r\n",
        "                    'здравствуйте','здрасте','здравчтвуйтет','здрастивити',\r\n",
        "                    'здравствуй','здрастувуйте','здраствутйте','здрсти','здрьте',\r\n",
        "                    'вопросздравствуйте','введитездравствуйте','здрасьте','здрастуйте',\r\n",
        "                    'здраастауйте','введздравствуйте','здравсте','привет','приветствую'\r\n",
        "                    'или','который','очень','еще','это']\r\n",
        "\r\n",
        "alphabet_en = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\r\n",
        "alphabet_ru = ['а','б','в','г','д','е','ё','ж','з','и','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц','ч','ш','щ','ъ','ы','ь','э','ю','я']\r\n",
        "layout_en = ['q','w','e','r','t','y','u','i','o','p','[',']','a','s','d','f','g','h','j','k','l',';','','z','x','c','v','b','n','m',',','.','`']\r\n",
        "layout_ru = ['й','ц','у','к','е','н','г','ш','щ','з','х','ъ','ф','ы','в','а','п','р','о','л','д','ж','э','я','ч','с','м','и','т','ь','б','ю','ё']\r\n",
        "\r\n",
        "# Словарь английских слов (для проверки на корректность раскладки)\r\n",
        "nltk.download('words')\r\n",
        "vocab_en = set(w.lower() for w in words.words())\r\n",
        "\r\n",
        "model_spell = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGlb1QoJpZBB"
      },
      "source": [
        "# Экземпляр класса pymorphy2\r\n",
        "morph_ru = pymorphy2.MorphAnalyzer()\r\n",
        "\r\n",
        "# Экземпляр класса pymystem3 (в разы медленнее)\r\n",
        "lemmatizator = Mystem()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQaw4HmMshC3"
      },
      "source": [
        "# Параметры предобработки\r\n",
        "do_clen = True\r\n",
        "do_lower_case = False # оставляем регистр без изменений т.к. будем использовать BERT модель обученную на данных без перевода в нижний регистр\r\n",
        "replace_empty_label = True\r\n",
        "do_tokenize = True\r\n",
        "do_fix_spell = False\r\n",
        "do_lemm = True\r\n",
        "lemm_by = 'pymorphy' # pymorphy, pymystem\r\n",
        "del_stops = True\r\n",
        "drop_dupl = True\r\n",
        "fix_case = True\r\n",
        "fix_layout = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq8omSBds6uE"
      },
      "source": [
        "## Принты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Jccfz_suCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eec281e-e8e3-46ed-cd8b-2a8e29416af8"
      },
      "source": [
        "# Заменяем пропуски\r\n",
        "print('Заменяем пропуски..')\r\n",
        "fillna(train_df,'text',replace_to='')\r\n",
        "fillna(eval_df,'text',replace_to='')\r\n",
        "    \r\n",
        "# Удаляем лишние символы\r\n",
        "if do_clen:\r\n",
        "    print('Удаляем лишние символы..')\r\n",
        "    train_df['text'] = train_df['text'].apply(clear_chars)\r\n",
        "    eval_df['text'] = eval_df['text'].apply(clear_chars)\r\n",
        "\r\n",
        "    \r\n",
        "# Переводим в нижний регистр\r\n",
        "if do_lower_case:\r\n",
        "    print('Переводим в нижний регист..')\r\n",
        "    train_df['text'] = train_df['text'].apply(do_lower)\r\n",
        "    eval_df['text'] = eval_df['text'].apply(do_lower)\r\n",
        "\r\n",
        "\r\n",
        "# Определяем наиболее частую метку для пропусков и присваиваем её для всех пропусков в тренировочных данных\r\n",
        "if replace_empty_label:\r\n",
        "    print('Заменяем пустые метки..')\r\n",
        "    replace_empty_to_max_freq_label(train_df,text_str='text',label_str='label',empty_str='')\r\n",
        "    replace_empty_to_max_freq_label(eval_df,text_str='text',label_str='label',empty_str='')\r\n",
        "    \r\n",
        "# Токенизируем текст (отделяем числа от текста и английский текст от русского)\r\n",
        "if do_tokenize:\r\n",
        "    print('Токенизация..')\r\n",
        "    train_df['text'] = train_df['text'].apply(tokenize_by_rules)\r\n",
        "    eval_df['text'] = eval_df['text'].apply(tokenize_by_rules)\r\n",
        "\r\n",
        "\r\n",
        "# Формируем список стоп-слов (слова которые встречаются в заданном числе меток)\r\n",
        "if del_stops:\r\n",
        "    print('Формируем список стоп-слов..')\r\n",
        "    stop_words = creat_stop_words(train_df,'text','labels',\r\n",
        "                                  stop_words_excep,\r\n",
        "                                  stop_words_extra,\r\n",
        "                                  min_labels_count=None)\r\n",
        "    stop_words = creat_stop_words(eval_df,'text','labels',\r\n",
        "                                  stop_words_excep,\r\n",
        "                                  stop_words_extra,\r\n",
        "                                  min_labels_count=None)\r\n",
        "\r\n",
        "\r\n",
        "    # Удаляем стоп-слова\r\n",
        "    print('Удаляем стоп-слова..')\r\n",
        "    train_df['text'] = train_df['text'].apply(delete_stopwords)\r\n",
        "    eval_df['text'] = eval_df['text'].apply(delete_stopwords)\r\n",
        "\r\n",
        "# Заменяем пустые строки на пробел (фикс ошибки при обучении BERT)\r\n",
        "print('Заменяем пустые строки на пробел..')\r\n",
        "train_df['text'] = train_df['text'].replace('',' ')\r\n",
        "eval_df['text'] = eval_df['text'].replace('',' ')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Заменяем пропуски..\n",
            "Удаляем лишние символы..\n",
            "Заменяем пустые метки..\n",
            "Токенизация..\n",
            "Формируем список стоп-слов..\n",
            "Удаляем стоп-слова..\n",
            "Заменяем пустые строки на пробел..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNqa1sMZttjA"
      },
      "source": [
        "# Сохранение датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeMmib3St0I3"
      },
      "source": [
        "train = train_df.to_csv('data/train.csv')\r\n",
        "eval = eval_df.to_csv('data/eval.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}